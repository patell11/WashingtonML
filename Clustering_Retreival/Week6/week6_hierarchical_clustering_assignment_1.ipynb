{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical clustering** refers to a class of clustering methods that seek to build a **hierarchy** of clusters, in which some clusters contain others. In this assignment, we will explore a top-down approach, recursively bipartitioning the data using k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans                # we'll be using scikit-learn's KMeans for this assignment\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mCLU06-NB01.ipynb\u001b[m\u001b[m\r\n",
      "DCphrd4eEemzYQ60aJAwVA_fc87f6033fa34a2ab644e47ef86539cc_em_utilities.py.zip\r\n",
      "EZW10-I8EemJfgqR2HI2sA_97cfb8979d704bf299ddc0b287ba68a6_CLU06-NB01.ipynb.zip\r\n",
      "_167fc84b78dc145609e919983b475aaa_people_wiki.csv.zip\r\n",
      "_395a4cfb2299d1655f1ef6bf6cc4f71b_people_wiki_tf_idf.npz.zip\r\n",
      "_96eadbec4d43a0b0870dde27d0652fb2_people_wiki_map_index_to_word.json.zip\r\n",
      "closing-annotated.pdf\r\n",
      "\u001b[1m\u001b[32mem_utilities.py\u001b[m\u001b[m\r\n",
      "i5zBDt4bEemhxxJUtZcQoA_7fb43aab245846f09a8c48977883f0b6_people_wiki.sframe.zip\r\n",
      "people_wiki.csv\r\n",
      "\u001b[1m\u001b[34mpeople_wiki.sframe\u001b[m\u001b[m\r\n",
      "people_wiki_map_index_to_word.json\r\n",
      "\u001b[1m\u001b[32mpeople_wiki_tf_idf.npz\u001b[m\u001b[m\r\n",
      "week6_hierarchical_clustering_assignment_1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki = pd.read_csv('people_wiki.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we extract the TF-IDF vector of each document.\n",
    "\n",
    "For your convenience, we extracted the TF-IDF vectors from the dataset. The vectors are packaged in a sparse matrix, where the i-th row gives the TF-IDF vectors for the i-th document. Each column corresponds to a unique word appearing in the dataset.\n",
    "\n",
    "To load in the TF-IDF vectors, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)\n",
    "\n",
    "tf_idf = load_sparse_csr('people_wiki_tf_idf.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('people_wiki_map_index_to_word.json', 'r') as filehandle:\n",
    "    map_index_to_word = json.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with the k-means assignment, let's normalize all vectors to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartition the Wikipedia dataset using k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our workflow for clustering text data with k-means:\n",
    "\n",
    "1. Load the dataframe containing a dataset, such as the Wikipedia text dataset.\n",
    "2. Extract the data matrix from the dataframe.\n",
    "3. Run k-means on the data matrix with some value of k.\n",
    "4. Visualize the clustering results using the centroids, cluster assignments, and the original dataframe. We keep the original dataframe around because the data matrix does not keep auxiliary information (in the case of the text dataset, the title of each article).\n",
    "\n",
    "Let us modify the workflow to perform bipartitioning:\n",
    "\n",
    "1. Load the dataframe containing a dataset, such as the Wikipedia text dataset.\n",
    "2. Extract the data matrix from the dataframe.\n",
    "3. Run k-means on the data matrix with k=2.\n",
    "4. Divide the data matrix into two parts using the cluster assignments.\n",
    "5. Divide the dataframe into two parts, again using the cluster assignments. This step is necessary to allow for visualization.\n",
    "6. Visualize the bipartition of data.\n",
    "\n",
    "We'd like to be able to repeat Steps 3-6 multiple times to produce a **hierarchy** of clusters such as the following:\n",
    "```\n",
    "                      (root)\n",
    "                         |\n",
    "            +------------+-------------+\n",
    "            |                          |\n",
    "         Cluster                    Cluster\n",
    "     +------+-----+             +------+-----+\n",
    "     |            |             |            |\n",
    "   Cluster     Cluster       Cluster      Cluster\n",
    "```\n",
    "Each **parent cluster** is bipartitioned to produce two **child clusters**. At the very top is the **root cluster**, which consists of the entire dataset.\n",
    "\n",
    "Now we write a wrapper function to bipartition a given cluster using k-means. There are three variables that together comprise the cluster:\n",
    "\n",
    "* `dataframe`: a subset of the original dataframe that correspond to member rows of the cluster\n",
    "* `matrix`: same set of rows, stored in sparse matrix format\n",
    "* `centroid`: the centroid of the cluster (not applicable for the root cluster)\n",
    "\n",
    "Rather than passing around the three variables separately, we package them into a Python dictionary. The wrapper function takes a single dictionary (representing a parent cluster) and returns two dictionaries (representing the child clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bipartition(cluster, maxiter=400, num_runs=4, seed=None):\n",
    "    '''cluster: should be a dictionary containing the following keys\n",
    "                * dataframe: original dataframe\n",
    "                * matrix:    same data, in matrix format\n",
    "                * centroid:  centroid for this particular cluster'''\n",
    "    \n",
    "    data_matrix = cluster['matrix']\n",
    "    dataframe   = cluster['dataframe']\n",
    "    \n",
    "    # Run k-means on the data matrix with k=2. We use scikit-learn here to simplify workflow.\n",
    "    kmeans_model = KMeans(n_clusters=2, max_iter=maxiter, n_init=num_runs, random_state=seed, n_jobs=1)    \n",
    "    kmeans_model.fit(data_matrix)\n",
    "    centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_\n",
    "    \n",
    "    # Divide the data matrix into two parts using the cluster assignments.\n",
    "    data_matrix_left_child, data_matrix_right_child = data_matrix[cluster_assignment==0], \\\n",
    "                                                      data_matrix[cluster_assignment==1]\n",
    "    \n",
    "    # Divide the dataframe into two parts, again using the cluster assignments.\n",
    "    dataframe['cluster_assignment'] = cluster_assignment # minor format conversion\n",
    "    dataframe_left_child, dataframe_right_child     = dataframe[dataframe['cluster_assignment']==0], \\\n",
    "                                                      dataframe[dataframe['cluster_assignment']==1]\n",
    "        \n",
    "    \n",
    "    # Package relevant variables for the child clusters\n",
    "    cluster_left_child  = {'matrix': data_matrix_left_child,\n",
    "                           'dataframe': dataframe_left_child,\n",
    "                           'centroid': centroids[0]}\n",
    "    cluster_right_child = {'matrix': data_matrix_right_child,\n",
    "                           'dataframe': dataframe_right_child,\n",
    "                           'centroid': centroids[1]}\n",
    "    \n",
    "    return (cluster_left_child, cluster_right_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs bipartitioning of the Wikipedia dataset. Allow 2+ minutes to finish.\n",
    "\n",
    "Note. For the purpose of the assignment, we set an explicit seed (`seed=1`) to produce identical outputs for every run. In pratical applications, you might want to use different random seeds for all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cc8a438034df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwiki_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'matrix'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataframe'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# no 'centroid' for the root cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mleft_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbipartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_runs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-903d2e9d7a87>\u001b[0m in \u001b[0;36mbipartition\u001b[0;34m(cluster, maxiter, num_runs, seed)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Run k-means on the data matrix with k=2. We use scikit-learn here to simplify workflow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mkmeans_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mkmeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_assignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amitpatel/anaconda/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                     \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                     x_squared_norms=x_squared_norms, random_state=seed)\n\u001b[0m\u001b[1;32m    938\u001b[0m                 \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amitpatel/anaconda/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             centers = _k_means._centers_sparse(X, sample_weight, labels,\n\u001b[0;32m--> 429\u001b[0;31m                                                n_clusters, distances)\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             centers = _k_means._centers_dense(X, sample_weight, labels,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Takes long time, do not run\n",
    "\n",
    "wiki_data = {'matrix': tf_idf, 'dataframe': wiki} # no 'centroid' for the root cluster\n",
    "left_child, right_child = bipartition(wiki_data, maxiter=100, num_runs=6, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'centroid': array([4.72706878e-06, 0.00000000e+00, 4.53832211e-06, ...,\n",
       "        1.10898073e-04, 9.02386276e-05, 2.11503833e-05]),\n",
       " 'dataframe':                                                      URI  \\\n",
       " 1           <http://dbpedia.org/resource/Alfred_J._Lewy>   \n",
       " 3      <http://dbpedia.org/resource/Franz_Rottensteiner>   \n",
       " 5            <http://dbpedia.org/resource/Sam_Henderson>   \n",
       " 7          <http://dbpedia.org/resource/Trevor_Ferguson>   \n",
       " 9             <http://dbpedia.org/resource/Cathy_Caruth>   \n",
       " ...                                                  ...   \n",
       " 59061             <http://dbpedia.org/resource/Rod_Wilt>   \n",
       " 59062  <http://dbpedia.org/resource/Scott_Baker_(judge)>   \n",
       " 59063  <http://dbpedia.org/resource/Dragoljub_Ojdani%...   \n",
       " 59064            <http://dbpedia.org/resource/Oz_Bengur>   \n",
       " 59070         <http://dbpedia.org/resource/Fawaz_Damrah>   \n",
       " \n",
       "                          name  \\\n",
       " 1              Alfred J. Lewy   \n",
       " 3         Franz Rottensteiner   \n",
       " 5               Sam Henderson   \n",
       " 7             Trevor Ferguson   \n",
       " 9                Cathy Caruth   \n",
       " ...                       ...   \n",
       " 59061                Rod Wilt   \n",
       " 59062     Scott Baker (judge)   \n",
       " 59063  Dragoljub Ojdani%C4%87   \n",
       " 59064               Oz Bengur   \n",
       " 59070            Fawaz Damrah   \n",
       " \n",
       "                                                     text  cluster_assignment  \n",
       " 1      alfred j lewy aka sandy lewy graduated from un...                   0  \n",
       " 3      franz rottensteiner born in waidmannsfeld lowe...                   0  \n",
       " 5      sam henderson born october 18 1969 is an ameri...                   0  \n",
       " 7      trevor ferguson aka john farrow born 11 novemb...                   0  \n",
       " 9      cathy caruth born 1955 is frank h t rhodes pro...                   0  \n",
       " ...                                                  ...                 ...  \n",
       " 59061  rod wilt is a former republican member of the ...                   0  \n",
       " 59062  sir thomas scott gillespie baker born 10 decem...                   0  \n",
       " 59063  dragoljub ojdani serbian cyrillic born june 1 ...                   0  \n",
       " 59064  osman oz bengur born february 23 1949 is an am...                   0  \n",
       " 59070  fawaz mohammed damrah arabic fawwz damra was t...                   0  \n",
       " \n",
       " [30273 rows x 4 columns],\n",
       " 'matrix': <30273x547979 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 5292869 stored elements in Compressed Sparse Row format>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'centroid': array([0.00000000e+00, 3.42736698e-06, 0.00000000e+00, ...,\n",
       "        1.20952547e-04, 7.96999832e-05, 2.04635492e-05]),\n",
       " 'dataframe':                                                      URI  \\\n",
       " 0            <http://dbpedia.org/resource/Digby_Morrell>   \n",
       " 2            <http://dbpedia.org/resource/Harpdog_Brown>   \n",
       " 4                   <http://dbpedia.org/resource/G-Enka>   \n",
       " 6            <http://dbpedia.org/resource/Aaron_LaCrate>   \n",
       " 8             <http://dbpedia.org/resource/Grant_Nelson>   \n",
       " ...                                                  ...   \n",
       " 59065  <http://dbpedia.org/resource/Dee_Brown_(basket...   \n",
       " 59066           <http://dbpedia.org/resource/Olari_Elts>   \n",
       " 59067       <http://dbpedia.org/resource/Scott_F._Crago>   \n",
       " 59068  <http://dbpedia.org/resource/David_Cass_(footb...   \n",
       " 59069          <http://dbpedia.org/resource/Keith_Elias>   \n",
       " \n",
       "                                     name  \\\n",
       " 0                          Digby Morrell   \n",
       " 2                          Harpdog Brown   \n",
       " 4                                 G-Enka   \n",
       " 6                          Aaron LaCrate   \n",
       " 8                           Grant Nelson   \n",
       " ...                                  ...   \n",
       " 59065  Dee Brown (basketball, born 1968)   \n",
       " 59066                         Olari Elts   \n",
       " 59067                     Scott F. Crago   \n",
       " 59068            David Cass (footballer)   \n",
       " 59069                        Keith Elias   \n",
       " \n",
       "                                                     text  cluster_assignment  \n",
       " 0      digby morrell born 10 october 1979 is a former...                   1  \n",
       " 2      harpdog brown is a singer and harmonica player...                   1  \n",
       " 4      henry krvits born 30 december 1974 in tallinn ...                   1  \n",
       " 6      aaron lacrate is an american music producer re...                   1  \n",
       " 8      grant nelson born 27 april 1971 in london also...                   1  \n",
       " ...                                                  ...                 ...  \n",
       " 59065  decovan kadell dee brown born november 29 1968...                   1  \n",
       " 59066  olari elts born april 27 1971 in tallinn eston...                   1  \n",
       " 59067  scott francis crago born july 26 1963 twin bro...                   1  \n",
       " 59068  david william royce cass born 27 march 1962 in...                   1  \n",
       " 59069  keith hector elias born february 3 1972 in lac...                   1  \n",
       " \n",
       " [28798 rows x 4 columns],\n",
       " 'matrix': <28798x547979 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 5086414 stored elements in Compressed Sparse Row format>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the bipartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with a modified version of the visualization function from the k-means assignment. For each cluster, we print the top 5 words with highest TF-IDF weights in the centroid and display excerpts for the 8 nearest neighbors of the centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_single_tf_idf_cluster(cluster, map_index_to_word):\n",
    "    '''map_index_to_word: SFrame specifying the mapping betweeen words and column indices'''\n",
    "    \n",
    "    wiki_subset   = cluster['dataframe']\n",
    "    tf_idf_subset = cluster['matrix']\n",
    "    centroid      = cluster['centroid']\n",
    "    \n",
    "    # Print top 5 words with largest TF-IDF weights in the cluster\n",
    "    idx = centroid.argsort()[::-1]\n",
    "    for i in range(5):\n",
    "        print('{0}:{1:.3f}'.format({v:k for k, v in map_index_to_word.items()}[idx[i]], centroid[idx[i]])),\n",
    "    print('')\n",
    "    \n",
    "    # Compute distances from the centroid to all data points in the cluster.\n",
    "    distances = pairwise_distances(tf_idf_subset, [centroid], metric='euclidean').flatten()\n",
    "    # compute nearest neighbors of the centroid within the cluster.\n",
    "    nearest_neighbors = distances.argsort()\n",
    "    # For 8 nearest neighbors, print the title as well as first 180 characters of text.\n",
    "    # Wrap the text at 80-character mark.\n",
    "    for i in range(8):\n",
    "        text = ' '.join(wiki_subset.iloc[nearest_neighbors[i]]['text'].split(None, 25)[0:25])\n",
    "        print('* {0:50s} {1:.5f}\\n  {2:s}\\n  {3:s}'.format(wiki_subset.iloc[nearest_neighbors[i]]['name'],\n",
    "              distances[nearest_neighbors[i]], text[:90], text[90:180] if len(text) > 90 else ''))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546431"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_index_to_word['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the two child clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she:0.022\n",
      "university:0.015\n",
      "her:0.013\n",
      "he:0.012\n",
      "served:0.010\n",
      "\n",
      "* Kayee Griffin                                      0.97357\n",
      "  kayee frances griffin born 6 february 1950 is an australian politician and former australi\n",
      "  an labor party member of the new south wales legislative council serving\n",
      "* %C3%81ine Hyland                                   0.97368\n",
      "  ine hyland ne donlon is emeritus professor of education and former vicepresident of univer\n",
      "  sity college cork ireland she was born in 1942 in athboy co\n",
      "* Christine Robertson                                0.97372\n",
      "  christine mary robertson born 5 october 1948 is an australian politician and former austra\n",
      "  lian labor party member of the new south wales legislative council serving\n",
      "* Anita Kunz                                         0.97468\n",
      "  anita e kunz oc born 1956 is a canadianborn artist and illustratorkunz has lived in london\n",
      "   new york and toronto contributing to magazines and working\n",
      "* Barry Sullivan (lawyer)                            0.97491\n",
      "  barry sullivan is a chicago lawyer and as of july 1 2009 the cooney conway chair in advoca\n",
      "  cy at loyola university chicago school of law\n",
      "* Margaret Catley-Carlson                            0.97532\n",
      "  margaret catleycarlson oc born 6 october 1942 is a canadian civil servant she was chair an\n",
      "  d is now a patron of the global water partnership\n",
      "* Vanessa Gilmore                                    0.97577\n",
      "  vanessa diane gilmore born october 1956 is a judge on the united states district court for\n",
      "   the southern district of texas she was appointed to\n",
      "* James A. Joseph                                    0.97626\n",
      "  james a joseph born 1935 is an american former diplomatjoseph is professor of the practice\n",
      "   of public policy studies at duke university and founder of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_single_tf_idf_cluster(left_child, map_index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she:0.023\n",
      "music:0.017\n",
      "her:0.017\n",
      "league:0.016\n",
      "season:0.016\n",
      "\n",
      "* Patricia Scott                                     0.97144\n",
      "  patricia scott pat born july 14 1929 is a former pitcher who played in the allamerican gir\n",
      "  ls professional baseball league for parts of four seasons\n",
      "* Madonna (entertainer)                              0.97185\n",
      "  madonna louise ciccone tkoni born august 16 1958 is an american singer songwriter actress \n",
      "  and businesswoman she achieved popularity by pushing the boundaries of lyrical\n",
      "* Janet Jackson                                      0.97261\n",
      "  janet damita jo jackson born may 16 1966 is an american singer songwriter and actress know\n",
      "  n for a series of sonically innovative socially conscious and\n",
      "* Natashia Williams                                  0.97346\n",
      "  natashia williamsblach born august 2 1978 is an american actress and former wonderbra camp\n",
      "  aign model who is perhaps best known for her role as shane\n",
      "* Todd Williams                                      0.97381\n",
      "  todd michael williams born february 13 1971 in syracuse new york is a former major league \n",
      "  baseball relief pitcher he attended east syracuseminoa high school\n",
      "* Marilyn Jenkins                                    0.97432\n",
      "  marilyn a jenkins jenks born september 18 1934 is a former catcher who played in the allam\n",
      "  erican girls professional baseball league listed at 5 ft\n",
      "* Kayla Bashore Smedley                              0.97498\n",
      "  kayla bashore born february 20 1983 in daegu south korea is an american field hockey defen\n",
      "  der and midfielder now living in san diego california she\n",
      "* Cher                                               0.97513\n",
      "  cher r born cherilyn sarkisian may 20 1946 is an american singer actress and television ho\n",
      "  st described as embodying female autonomy in a maledominated industry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_single_tf_idf_cluster(right_child, map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right cluster consists of athletes and artists (singers and actors/actresses), whereas the left cluster consists of non-athletes and non-artists. So far, we have a single-level hierarchy consisting of two clusters, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "                                           Wikipedia\n",
    "                                               +\n",
    "                                               |\n",
    "                    +--------------------------+--------------------+\n",
    "                    |                                               |\n",
    "                    +                                               +\n",
    "         Non-athletes/artists                                Athletes/artists\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this hierarchy good enough? **When building a hierarchy of clusters, we must keep our particular application in mind.** For instance, we might want to build a **directory** for Wikipedia articles. A good directory would let you quickly narrow down your search to a small set of related articles. The categories of athletes and non-athletes are too general to facilitate efficient search. For this reason, we decide to build another level into our hierarchy of clusters with the goal of getting more specific cluster structure at the lower level. To that end, we subdivide both the `athletes/artists` and `non-athletes/artists` clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
